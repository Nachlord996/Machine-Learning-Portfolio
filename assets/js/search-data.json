{
  
    
        "post0": {
            "title": "Un problema clásico de clasificación: UCI Iris mediante K-NN",
            "content": "Dentro de las técnicas para clasificación en problemas no lineales, existe un grupo que utiliza la distancia entre predictores como elemento central. Esto parte de la idea de que los ejemplos que pertenecen a una misma clase, serán ciertamente más similares en cuanto a valores de los distintos predictores. . K-nearest Neighbours o simplemente K-NN, es un algoritmo para problemas de clasificación que se basa en almacenar la información del dataset y evaluar nuevos ejemplos a partir de un cálculo simple de distancia. La clase predicha de un elemento estará dada por los K elementos (previamente clasificados) más cercanos. . Contexto . Para este ejercicio se utilizará como base el Dataset Iris de UCI, el mismo es de acceso totalmente público y gratuito. En Rapidminer generaré un flujo que entrene un modelo a partir de los datos de Iris, en este caso bajo la proporción 70/30, para luego testearlo, revisar los resultados y las distintas opciones de configuración que el modelo ofrece. . Datos . El Dataset Iris es bastante reducido, cuenta solamente con 150 ejemplos. Además de la variable objetivo, existen 4 predictores distintos relacionados con medidas de las flores observadas en la realidad. . El dataset está completamente etiquetado y ningún predictor tiene valores faltantes. La siguiente tabla resume las variables disponibles en el Dataset: . Variable Tipo de dato . Largo Sépalo | Numérico | . Ancho del Sépalo | Numérico | . Largo del Pétalo | Numérico | . Ancho del Pétalo | Numérico | . La variables objetivo es del tipo Polinomial y tiene 3 valores posibles: Iris Setosa, Iris Versi-color e Iris Virginica. . Examinando los gráficos de tipo plot entre los predictores, resulta interesante la agrupación generada en el espacio de las clases para las variables “Largo Pétalo” y “Ancho Pétalo”. Además, estos dos predictores son los que tienen mayor correlación con la variable objetivo. . . Modelo . En Rapidminer, se importa el Dataset de Iris. En primer lugar, es necesario eliminar los predictores que no vamos a utilizar para el ejercicio. Luego, se realiza la separación de los datos mediante muestreo aleatorio en 70% para Entrenamiento y 30% para Test, esto se puede realizar mediante el operador “Split Data”. . Agregamos el operador de K-NN con la configuración que viene por defecto, seguido de un “Apply Model” para predecir los valores de la clase objetivo para el 30% reservado para Test. . El flujo debería verse así: . . Evaluación . Al ejecutar el modelo, vemos que los resultados obtenidos son muy positivos. Esto quiere decir que nuestro modelo aprovechó bien la correlación entre predictores y la separación en el hyperespacio de las clases objetivo. . Aquí la matriz de confusión para esta ejecución inicial: . Vale destacar que el operador de K-NN ofrece distintos métodos para el cálculo de la distincia, los cuales podrían ser interesantes para otras situaciones donde la separación no sea tan clara o el resultado obtenido no sea tan bueno. . Distancia de Chebychev | Distancia Euclideana | Similitud Coseno | Distancia de Manhattan | . También, Rapidminer ofrece la posibilidad de ponderar el voto en la selección de la clase objetivo. Esto se puede establecer mediante un switch el cual determina si la distancia importa en la decisión. Es decir, aquellos vecinos cercanos tendrán una mayor incidencia en la clase escogida. .",
            "url": "https://nachlord996.github.io/Machine-Learning-Portfolio/k%20nearest%20neighbours/rapidminer/no%20lineal/2021/12/01/KNN-Clasificaci%C3%B3n-del-Dataset-Iris.html",
            "relUrl": "/k%20nearest%20neighbours/rapidminer/no%20lineal/2021/12/01/KNN-Clasificaci%C3%B3n-del-Dataset-Iris.html",
            "date": " • Dec 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Revisión de técnincas de aprendizaje no supervisado: K-means",
            "content": "Los métodos de aprendizaje automático se pueden dividir en dos grandes enfoques, los supervisados y los no supervisados. El primero se basa en el uso de Datasets etiquetados. Esta información es utilizado en el entrenamiento de un modelo para que este aprenda las relaciones que existen entre los datos de entrada y una variable objetivo. Por esto mismo, este enfoque se centra en generar la capacidad de predicción a partir de ejemplos nuevos, con una precisión aceptable. . Por otra parte, los algoritmos de Machine Learning no supervisados tienen el objetivo de trabajar con datos no etiquetados. No existe una variable objetivo a predecir ni un comportamiento en particular para simular. Este tipo de algoritmos se caracterizan por encontrar patrones y otorgar mayor información sobre un conjunto de datos existente. Dentro de este conjunto se encuentra el algoritmo a tratar en este artículo: K-Means. . K-Means es un algoritmo no supervisado de Clustering. Está pensado para descubrir los distintos grupos que puedan existir en un Dataset sin ese conocimiento previamente. . Contexto . En este ejercicio se cuenta una base de datos de pacientes de un hospital. Se desea obtener más información acerca de la influencia del peso, el sexo y el colesterol en el desarrollo de una enfermedad coronaria. . Esta tarea implica detectar la existencia de grupos naturales en los datos que relacionen estos factores mencionados con el desarrollo de la enfermedad. El modelo se entrenará utilizando la herramiento Rapidminer. . Datos . Se cuenta con un dataset con 3 variables, dos de estas numércias y una binomial. . Sexo | Peso | Colesterol | . El dataset tiene 547 registros y no hay valores faltantes. Los datos parecen ser correctos y estar expresados en las unidades adecuadas. . . Modelo . En el flujo de Rapidminer se debe agregar un operador K-means que entrenará el modelo de clustering. El mismo necesita un parámetro k que significa la cantidad de clusters a generar. Por conocimiento previo del Dataset, se ingresará el valor K = 4, aunque esto normalmente se decide en base a la exploración realizada del Dataset. . Además se puede insertar un operador de visualización de clústeres para poder entender mejor los resultados generados por el modelo. El entrenamiento determina una lista de centroides, puntos en el hyperespacio que representan los centros de los grupos generados. Los demás puntos determinan el grupo al cual pertenecen por medio de la distancia a los centroides. . El flujo se ve de la siguiente manera: . . La salida que genera el último operador aporta un gráfico de Scatter Plot que muestra visualmente los grupos generados. La utilidad de este modelo podría ser identificar personas que pertenezcan a los clústers de mayor peso y colesterol, ya que estos son los más propensos a desarrollar una enfermedad cardíaca. . .",
            "url": "https://nachlord996.github.io/Machine-Learning-Portfolio/clustering/k-means/rapidminer/2021/11/28/Revision-de-t%C3%A9cnicas-de-aprendizaje-no-supervisado-k-means.html",
            "relUrl": "/clustering/k-means/rapidminer/2021/11/28/Revision-de-t%C3%A9cnicas-de-aprendizaje-no-supervisado-k-means.html",
            "date": " • Nov 28, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Validación de un modelo: Respuesta a una campaña de mail usando Naïve Bayes",
            "content": "La calidad de un modelo de Machine Learning no se evalúa exclusivamente con los resultados que este genera durante su entrenamiento y validación. La aplicabilidad de cualquier modelo debe considerar múltiples métricas que demuestren su efectividad en la realidad. . Evaluar incorrectamente un modelo podría generar expectativas irrealistas de su funcionamiento, las cuales harán aparecer problemas en la fase de producción, al utilizar el modelo con datos de la realidad. . Por tanto, el desempeño real de un modelo podría no ser aceptable y en muchos casos generar daños elevados para los clientes, incluyendo dinero y vidas. Un error frecuente de los analistas, es sobredimensionar el funcionamiento de un modelo en base al porcentaje de precisión que arroje. . El objetivo de este documento será evaluar distintas técnicas de validación de modelos de Machine Learning para un caso particular. . Escenario . Para este caso, utilizaremos la herramienta de Rapidminer para generar datos de ejemplo. El objetivo es predecir si una persona responderá a una campaña de marketing por mail utilizando atributos demográficos. . Para ello, existe un operador en Rapidminer “Generate Direct Mailing Data”. Este operador utiliza dos parámetros: . Número de ejemplos a generar. En este caso usaré 10000. | Semilla local. Activar este parámetro me permitirá repetir el ejercicio manteniendo los datos generados. | . Preparación de Datos . El operador de generación arroja un dataset del tamaño indicado con los siguientes predictores: . Nombre | Edad | Estilo de vida (Tres categorías: Activo, Cómodo o Saludable) | Código Postal | Estado Civil | Tipo de vehículo (Dos categorías: Práctico o Costoso) | Deporte (Tres deportes distintos: Bádminton, Fútbol y Atletismo) | Salario | Respuesta al mail | . Antes de comenzar a trabajar, fue necesario realizar algunos ajustes. En primer lugar, la variable a predecir (Si la persona realmente responderá a la campaña por mail) tiene dos posibles valores Response / No Responde, por lo que se puede tratar esta variable como Binomial. . El dataset completo está etiquetado, por lo que podremos dividirlo en dos para separar en Entrenamiento y Test. Aplicaremos la regla del 80/20 y además configuraremos un Split Validation para el dataset de entrenamiento. En este caso, la validación tendrá una proporción 70/30. . El flujo resultante es el siguiente: . . Modelo . Al enfrentarnos a un problema supervisado de clasificación binaria, disponemos de una gran variedad de modelos para probar. Sin embargo, como el objetivo es analizar las métricas de Performance, optaremos por un modelo sencillo de Naive Bayes sin pensar mucho en cuál será el más adecuado. . Dentro del operador de Split Validation, ubiqué el operador de Naive Bayes, aplicación del modelo y luego un operador de Performance activando las siguientes opciones: . Accuracy | False positive | False negative | True positive | True negative | Sensivity | Specificity | AUC | . Por último, utilicé el dataset reservado como Hold-out para testear el modelo generado. A nivel principal del flujo, se debe agregar un operador de Apply Model con el Dataset de test. . Como última medida de performance, agregué una Lift Chart a la salida del Apply Model. Este gráfico apuntará al valor de la clase a predecir “Response”. . Resultados . El proceso de Rapidminer genera la siguiente matriz de confusión para los resultados del dataset de entrenamiento: .   Response reales No Response reales . Response Predichos | 56 | 33 | . No Response Predichos | 15 | 136 | . Si tomamos como clase positiva a los valores “Response”, obtendremos los siguientes datos: . Falsos positivos (FP): 33 | Falsos negativos (FN): 15 | Verdaderos positivos (TP): 56 | Verdaderos negativos (TN): 136 | . Para estos valores, podemos verificar las métricas teóricas y contrastarlas con los resultados de Rapidminer. . Sensibilidad: TP / (TP + FN) 56 / (56 + 15) = 56 / 71 = 0.788 =&gt; 78.8% . Especificidad: TN / (TN + FP) 136 / (136 + 33) = 136 / 169 = 0.804 = 80.4% . Estos valores son idénticos a los calculados automáticamente por Rapidminer: . . Resulta interesante analizar los valores de Class Recall. Puntualmente, estos valores representan la proporción en que una clase dada de los datos fue predicha correctamente. . Este valor tiende a ser influenciado por la representación de la clase en el Dataset. Es decir, el modelo podrá comportarse adecuadamente en general, pero muy deficientemente para una clase en particular. Este efecto se maximiza cuando otras clases están sobrerepresentadas en el Dataset. Cuando esto sucede, se dice que el Dataset está desbalanceado. . En este caso, el Recall para los “No Response” es mayor que el Recall para los “Response”. Si analizamos la representación de cada clase en el dataset inicial, podemos ver que realmente existe una relación entre los valores resultantes y la proporción de las clases. . . Con respecto a la curva ROC y el valor AUC, podemos decir que el modelo se comporta relativamente bien. El valor resultante del área bajo la curva es de 0.861. Supera el valor de la elección aleatoria, donde el área sería de 0.5. . . Tomando en consideración este mismo concepto de mejora sobre la selección aleatoria, la confianza de predicción del modelo va a ser distinta para cada uno de los valores que clasifica. . A partir de esta información, centrándonos en los verdaderos positivos, es posible construir un gráfico que muestre como varía la mejora del modelo en función de la confianza. . De esta manera, es posible saber la mejora sobre la elección aleatoria, en definitiva, que tan bien funciona un modelo, tomando parte de la población total. . En este caso encontré que la cobertura alcanza el 70% en la tercera barra del gráfico. Este dato es interesante porque significaría que un 30% de la población es necesaria para lograr un 70%. . .",
            "url": "https://nachlord996.github.io/Machine-Learning-Portfolio/na%C3%AFve%20bayes/validaci%C3%B3n%20del%20modelo/roc/lift%20charts/2021/11/28/Respuesta-a-una-campa%C3%B1a-de-mail-usando-Naive-Bayes.html",
            "relUrl": "/na%C3%AFve%20bayes/validaci%C3%B3n%20del%20modelo/roc/lift%20charts/2021/11/28/Respuesta-a-una-campa%C3%B1a-de-mail-usando-Naive-Bayes.html",
            "date": " • Nov 28, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Caso de Estudio: Detección de enfermedad cardíaca",
            "content": "Contexto del problema . Me resulta particularmente interesante las posibilidades de aplicación de la Ciencia de Datos en el área de la Medicina. Los Hospitales y centros médicos en general, son instituciones que generan y registran información sobre el estado de sus pacientes diariamente. Por lo que, en principio es un buen campo de investigación para proyectos de esta índole. . . La información puede ser aprovechada mediante técnicas de Machine Learning para el entrenamiento de modelos que aporten valor a la labor médica. Sin embargo, la mayoría de las instituciones no estarán dispuestas a compartir dicha información para proteger así la privacidad de sus usuarios. En definitiva, los pacientes depositan su confianza en estos centros médicos y tienen el derecho legítimo de que su información personal no sea filtrada. Este derecho se ve respaldado por la obligación, del otro lado del mostrador, de resguardar esta información. Esto último deriva en que la disponibilidad de información sea mayormente escasa y difícil de conseguir. . En este caso de estudio se pretende analizar información de pacientes para la detección de enfermedades cardíacas. Se parte de Datasets que están disponibles públicamente y han sido objeto de estudio en varios artículos sobre esta temática. Estos datos provienen de 4 bases de datos distintas de: . Cleveland Clinic Foundation | Hungarian Institute of Cardiology, Budapest | V.A. Medical Center, Long Beach, CA | University Hospital, Zurich, Switzerland | El objetivo principal es demostrar las habilidades aprendidas considerando todas las etapas de un proyecto de Machine Learning. . Preparación de los Datos . Primera aproximación . A pesar de que los Datasets tienen orígenes distintos, los atributos que los componen se mantienen en todos los Datasets. Por lo tanto, podemos afirmar que una posible unificación de los datos será correcta y beneficiosa para el estudio, ya que contaremos con mayor cantidad de ejemplos y todos tienen la misma estructura. Esto implica también, que los datos registrados estén en las mismas unidades y encodeados de la misma manera. Diversos tipos de datos pueden tener el mismo significado pero tener distinta representación. Durante la preparación de los datos, debemos prestar especial atención a este aspecto ya que es uno de los errores más frecuentes y es fácil pasar por alto. . Como primera aproximación, deberemos investigar la estructura de los archivos, para luego poder levantarlos correctamente en la herramienta escogida. . Cada uno de los extractos de las bases de datos estan en archivos individuales .DATA. Existe además un archivo .NAMES que detalla la información presente en los anteriormente mencionados. . . La estructura de los datos no parece ser la más conveniente para su lectura, por lo que surge la necesidad de realizar transformaciones de formato. Para ello, realizaremos un script que convierta cada Dataset a un archivo .csv independiente. . Reestructuración de archivos de datos . Analizando la estructura de este archivo encuentro un patrón, cada cierta cantidad de líneas (En principio 10) hay una que finaliza la palabra “name”. De acuerdo con el archivo “.Names” asociado a estos Datasets, esta variable representa el último atributo del Dataset, por lo que podría ser utilizado para delimitar registros. En el caso de todos los valores intermedios, cada uno está asociado a un predictor según el orden descripto en el archivo “.Names”. . Vale destacar también, que el primer valor de cada registro se ve incrementando secuencialmente y es distinto para cada uno de los grupos, por lo que coincide con la descripción de Identificador que se esperaba. Los identificadores son un tipo de datos que no debe ser incluido en el proceso de entrenamiento de un modelo. El mismo no aporta información relevante para el problema que se está analizando. La utilidad que aporta este dato es exclusivamente referencial, para que el lector pueda identificar registros en particular. . El script fue escrito en Javascript y ejecutado en un entorno local de NodeJS. A continuación se presenta el código utilizado para estandarizar la información en un formato legible y universal CSV. El script fue ejecutado para cada uno de los archivos del caso de estudio. Simplemente se leen los archivos en busca de la palabra clave “name”, mientras que se van acumulando los valores obtenidos hasta el momento. Al encontrar esta palabra, se verifica la integridad del registro encontrado para decidir si será considerado o no. Por último se escriben las líneas en el archivo .csv de salida. . . Vale destacar que algunos ejemplos fueron retirados del Dataset final. Esto se debe a que contenían caracteres inválidos para los predictores que se están manejando o la cantidad de datos superaba lo necesario por cada ejemplo. Esta técnica supone una decisión sobre los datos, la cual debe ser tomada con precaución. En este caso, los ejemplos con problemas de encoding en el archivo fueron omitidos porque suponían un porcentaje muy bajo de la muestra. . Dataset Ejemplos totales Ejemplos con errores Porcentaje de error . Cleveland | 290 | 8 | 2.75% | . Switzerland | 123 | 0 | 0% | . Hungarian | 294 | 0 | 0% | . Long Beach VA | 200 | 0 | 0% | . Ahora tenemos la información correctamente almacenada en 4 archivos CSV distintos. La siguiente imagen muestra el resultado de la ejecución del script para el dataset “Cleveland”. También es de utilidad incluir los encabezados de los datos. Los mismos están disponibles en el archivo .Names. . . Unificación de fuentes de datos . Para cargar el Dataset en Rapidminer, se agregarán las 4 fuentes de datos como archivos al repositorio local. Luego se recuperan en el flujo mediante el operador Retrieve. La idea es combinar estos datasets en un único dataset y analizar estadísticas sobre él. Esto se puede realizar aplicando el operador Join. . . Lo primero que se puede apreciar es la gran cantidad de predictores, se dispone de un total de 76. Dentro de la etapa de prepración de los datos también es relevante reducir la cantidad de predictores a utilizar en el modelo. Esto supone un beneficio desde el punto de vista computacional, ya que se deberá trabajar con datos dimensionalmente más simples y por lo tanto con menos requerimientos de memoria y cómputo. Por otra parte, algunos de estos predictores podrían estar correlacionados entre sí, lo cual influye sustancialmente en la estabilidad de algunos modelos. Además de un beneficio, la cantidad de atributos puede ser una restricción para ciertos modelos como K-NN ya que por cada predictor que se agrega, el tiempo de ejecución total aumenta exponencialmente. . Análisis de valores faltantes . Por otra parte, múltiples predictores contienen una gran cantidad de valores faltantes. Existen diversas técnicas para combatir los valores faltantes de un dataset, conformando lo que se conoce como imputación de valores. En este caso, usaremos el criterio de valores de faltantes en relación al total de ejemplos para reducir los predictores a utilizar. Esto se debe a que será muy difícil obtener imputaciones semejantes a la realidad cuando los valores faltantes predominan para ese predictor. . La decisión es eliminar aquellos predictores que tengan 33% o más de valores faltantes. Para este caso, si el predictor tiene 300 o más ejemplos con valores faltantes, será directamente eliminado del Dataset. Analizando este criterio, los predictores a eliminar son estos: . Predictor # Valores Faltantes . slope | 308 | . cigs | 420 | . famhist | 422 | . rldv5 | 425 | . years | 432 | . thaltime | 453 | . thal | 477 | . diag | 558 | . ramus | 567 | . om2 | 572 | . cathef | 588 | . ca | 608 | . smoke | 669 | . thalsev | 769 | . junk | 780 | . dm | 804 | . thalpul | 898 | . restwm | 869 | . restef | 871 | . exerwm | 894 | . exeref | 897 | . earlobe | 898 | . exerckm | 898 | . restckm | 899 | . pncaden | 899 | . Esta operación se puede realizar mediante el operador Select Attributes indicando manualmente los predictores a escoger. Esta decisión nos genera una nueva versión del Dataset con 45 Predictores para trabajar. Realmente serán 44, porque debemos quitar el atributo ID antes de empezar a trabajar. Rapidminer ofrece dos opciones para este tipo de dato, se puede ajustar el rol del predictor a “ID” o se puede directamente eliminar de la tabla. En este caso optaré por eliminarlo ya que no aporta información importante de identificación posterior. . El flujo hasta este punto se ve de esta manera: . . Imputación de valores . Con respecto a los restantes valores faltantes, aplicaremos una imputación clásica utilizando utilizando K-NN. Este algoritmo basado en distancias, reemplazará los valores que falten recopilando información de ejemplos similares. Vale destacar que el funcionamiento de K-NN parte de la base de que los tipos de datos van a estar bien asignados. En este paso revisamos uno a uno los atributos para asegurarnos de que están bien cargados. . Detección de outliers . También es importante analizar la distribución de valores para detectar posibles outliers. El primer paso en este análisis es constatar errores de medición o registro de información. Esto se da en el atributo “Prop”, donde debería existir una variable Binomial, se encuentra un ejemplo que tiene un valor de 22 para este predictor. Esto claremente se trata de outlier que no aporta información relevante al problema y solamente genera ruido. También sucede para la variable numérica “Lmt”. Existe un valor de 162 que se encuentra muy alejado del resto de la distribución. . En los siguientes gráficos, en escala logarítmica, se puede apreciar que solamente existe un registro con este valor outlier frente al resto de la distribución. . . . Para este último caso fue necesario variar la cantidad de bins generadas por el histograma para visualizar el outlier. . Estos ejemplos con outliers serán eliminados del Dataset . Los siguientes predictores tuvieron que ser ajustados en términos del tipo de dato: . Predictor Tipo de dato nuevo . dig | Binomial | . diuretic | Binomial | . exang | Binomial | . painloc | Binomial | . painexer | Binomial | . relrest | Binomial | . prop | Binomial | . nitr | Binomial | . htn | Binomial | . xhypo | Binomial | . pro | Binomial | . sex | Binomial | . num | Nominal | . restcg | Nominal | . cp | Nominal | . Feature selection . Para este caso, se ha decidido no utilizar técnicas de Feature Selection avanzadas como Análisis de Componentes Lineales. Por el contrario, se seleccionarán los predictores cuya correlación con la variable objetivo sea alta. Para ello, se definirá un coeficiente de umbral para evaluar si un predictor será utilizado, a partir de su valor de correlación. Es importante entender que este filtro servirá exclusivamente para predictores numéricos. El resto de los atributos permanecerán en el Dataset. . La correlación con la variable de salida puede verse por medio de la matriz de correlación. Tomando una correlación de por lo menos 0.14, es decir mayor a 0.14 y menos a -0.14, el conjunto de predictores que cumplen esta condición es este: . thalach | age | thaldur | proto | chol | thalrest | oldpeak | laddist | ladprox | lvx4 | met | lmt | lfv | lvx3 | painexer | rcadist | tpeakbps | tpeakbpd | . Ahora que tenemos el conjunto final de predictores a utilizar, es pertinente evaluar las distintas distribuciones que puedan tener. Este dato es importante especialmente para ciertos algoritmos que tienen un mejor rendimiento con distribuciones gaussianas. También es una oportunidad para evaluar la proporción de clases de la variable objetivo. . Transformaciones sobre los datos . . Claramente la primera clase tiene una sobrerepresentación en el Dataset. Los ejemplos que corresponden a esta clase son los pacientes cuya predicción indica que no sufrieron enfermedades cardíacas. . En los predictores seleccionados de tipo continuo no se observa un sesgo o Skewness muy pronunciado en ningún sentido. Por lo tanto, considero que los datos sin trasformaciones podrían ser utilizados en primera instancia para evaluar distintos modelos. De ser necesario ajustar alguno de ellos, existen diversas técnicas para transformar estos predictores. La raíz cuadrada, el logaritmo natural y la función inversa son probablemente las más utilizadas para obtener distribuciones más apropiadas, es decir, más parecidas a la normal. . Modelo . Elección del modelo . La generación del modelo depende en gran parte del tipo de problema que estemos atacando, esto implica lo que queremos conseguir con el modelo y también la naturaleza del problema. Para comenzar, estamos frente a un problema de clasificación supervisada. Esto se debe a que cada ejemplo en nuestro Dataset está etiquetado con el valor de una variable objetivo dada. . El objetivo del modelo será abstraer la relación existente entre los datos de entrada y la salida a partir de los valores presentes en el Dataset. Además, estamos buscando ubicar los nuevos registros en cada una de las clases de la variable objetivo, que en este caso son 5 distintas, eso significa que estamos ante un problema de clasificación multivariable. . Ante esta situación, creo que una buena decisión sería comenzar con un Árbol de Decisión. Luego de evaluar el modelo, optaría por avanzar a un modelo de ensamble de Random Forest y revisar las métricas con respecto al modelo anterior. . Separación de datos para Entrenamiento y Test . El entrenamiento del modelo se realizará con una parte mayoritaria del Dataset. Esto implica una división donde una parte se utilizará en el entrenamiento y el resto de ejemplos para probar el modelo generado. En este caso, optaré por seguir la regla del 70 / 30. La partición del Dataset puede ser realizada en Rapidminer mediante el operador “Split Data”. . La selección de ejemplos para cada una de las particiones debe tener un criterio, esta configuración se conoce como Sampling Type. En principio se ofrecen varias opciones en el operador de Split, las más recomendables son muestreo aleatorio y muestro estratificado. En este caso usaremos el estratificado para mantener la proporción de clases del Dataset original en estas nuevas particiones. . El flujo debería verse de la siguiente manera: . . El entrenamiento se realizará aplicando un K-Fold Cross Validation utilizando un valor de K estándar de 10. Esta técnica permite general un modelo más realista, aprovechando la información que aporta el Dataset al máximo. Esto se logra ejecutando distintas versiones del modelo y optimizando una métrica en particular. El proceso divide el dataset en K partes iguales y entrena usando K - 1 partes en cada iteración. Asegurar la independencia de los datasets de test es un factor clave para el éxito de esta técnica. . Dentro del operador de Cross Validation, ubicamos el operador de Árbol de decisión con los parámetros por defecto. En principio esto servirá para evaluar la performance general del modelo y encaminar una mejora del modelo utilizando este algoritmo de clasificación. . El modelo debería verse de la siguiente manera: . . Evaluación del Árbol de Decisión . Los resultados preliminares de performance indican un rendimiento aceptable. El principal componente a analizar en esta fase preliminar es la Matriz de Confusión. Aquí se presentan las distintas clases a predecir y la correspondencia con los valores predichos. De esta tabla pueden obtenerse los falsos positivos, los falsos negativos, los verdaderos positivos y verdaderos negativos. En el caso ideal, todos los valores predichos corresponderán con los valores reales, el modelo tendrá un 100% de precisión y también la matriz de confusión tendrá valores distintos de cero exclusivamente en la diagonal principal. . . Por medio de la matriz de confusión, se pueden obtener también métricas interesantes a analizar en un problema multiclase. La más destacada se llama Class Recall la proporción de casos en que el modelo predijo bien esa clase en particular. Este dato resulta importante ya que nos indica el nivel de funcionamiento del modelo para una clase en particular. . En este caso de estudio, la variable objetivo tendrá un valor entero entre 0 y 4, siendo las clases extremas las más relevantes. Aquellos pacientes que tengan un 0 en la predicción, tendrán una probabilidad muy baja de tener una enfermedad cardíaca. Lo contrario sucede para aquellos que obtienen 3 o 4, donde es muy importante que la situación sea detectada para poder tratar y prevenir mayores consecuencias. . El peor Recall obtenido es para la clase 2 que esté en medio, con un 16.28%. Sin embargo, esto no parecería ser tan crítico porque la mayoría de las predicciones erradas dieron un resultado entre 1 y 3, valores no muy extremos que se encuentran en esa zona intermedia de riesgo. . Debemos recordar que al principio nos encontramos con un Dataset desbalanceado, existían muchos más ejemplos de la clase 0 que de las demás clases. Esto es peligroso al momento de validar un modelo porque sesga el resultado de Accuracy. Si el modelo clasifica todos los ejemplos como la clase predominante, el error de algunos ejemplos (las clases menos representadas) significarán poco con respecto al total. Una herramienta útil para estas situaciones es el balanceo por pesos, donde los registros del entrenamiento se ponderan según la clase que tienen asignada. Este criterio de ponderación busca igualar la representación de clases en los ejemplos del Dataset. . Comparación con Random Forest . La mayor parte de los modelos de producción utilizan un enfoque conocido como Ensambles. Esta técnica tiene como objetivo reducir los errores de un modelo en particular con respecto a un dataset en particular. Es posible que la solución encontrada aprenda demasiado de un Dataset y temrine asimilando el ruido de los datos como información útil. Si bien localmente puede que la solución sea útil, no lo será en la generalidad del espacio de posibilidades. Entonces, resulta úlil evaluar la opinión que distintos modelos puedan tener para una predicción en particular. . De esta manera, se generan distintos algoritmos para elegir un resultado como respuesta, en base a las respuestas emitidas por los modelos individuales. Esto se puede generar de distintas maneras. Se podría tener una colección de modelos variando las siguientes características: . Ejemplos para entrenamiento del modelo | Algoritmo del modelo | Parámetros del modelo | Predictores del Dataset | . En este caso, aplicaré un Random Forest para evaluar si el rendimiento general del modelo obtiene una mejora. Para ello, reemplazo el operador de Árbol de decisión por el de Random Forest y configuro para que utilice un tamaño de 1000 árboles y el criterio de votación sea por confianza. . Estos son los resultados de la ejecución de ese modelo: . . Como se puede ver en la matriz de confusión, los distintos Class Recall mejoraron bastante en comparación con los resultados del árbol de decisión. Esto me aporta mayor confianza a la hora de utilizar este modelo de ensamble. . También vemos incrementada la precisión del modelo en general, ahora tenemos un 81.04%. .",
            "url": "https://nachlord996.github.io/Machine-Learning-Portfolio/caso%20de%20estudio/%C3%A1rbol%20de%20decisi%C3%B3n/random%20forest/rapidminer/preparaci%C3%B3n%20de%20los%20datos/entrenamiento/2021/11/25/Caso-de-Estudio-Predicci%C3%B3n-de-enfermedad-del-coraz%C3%B3n.html",
            "relUrl": "/caso%20de%20estudio/%C3%A1rbol%20de%20decisi%C3%B3n/random%20forest/rapidminer/preparaci%C3%B3n%20de%20los%20datos/entrenamiento/2021/11/25/Caso-de-Estudio-Predicci%C3%B3n-de-enfermedad-del-coraz%C3%B3n.html",
            "date": " • Nov 25, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Efecto De La Estandarización Y Normalización",
            "content": "Efecto de la estandarización y normalización . En la mayoría de los modelos de Machine Learning, es recomendable partir de datos estandarizados o normalizados para obtener un mejor desempeño del modelo a entrenar. Por esta razón, resulta interesante verificar el efecto de estas transformaciones previas en el resultado final del modelo. . Para evaluar empíricamente este efecto, se propone utilizar la herramienta Rapidminer y generar un flujo que valide la precisión de un modelo en particular. Para esto, el flujo deberá verificar la métrica, a partir de un modelo entrenado con datos estandarizados (o normalizar) y otro con datos sin estandarizar (o normalizar). . El dataset a utilizar será Wine UCI y el modelo a entrenar será del tipo clasificador Naive Bayes. . A los efectos de entender mejor la problemática, resulta conveniente explicitar la diferencia entre los conceptos “normalización” y “estandarización”. Si bien ambas técnicas implican llevar los predictores a una escala común, su implementación es distinta. La normalización se centra en el intervalo objetivo de la transformación, escalando los valores según su proporción original (p.e.: El intervalo 0.0 a 1.1). Por otra parte, la estandarización implica llevar los valores de los atributos a un conjunto con media cero y desviación estándar uno. . Para comenzar se crea un nuevo flujo en Rapidminer y se agregan los operadores de Naive Bayes. Cada uno de los operadores se nutren con el modelo Wine UCI, con el detalle de que uno de ellos recibe los datos normalizados utilizando “Transformación Z” en la configuración del operador. . . El modelo por evaluar no debería ser afectado por las escalas de los atributos, ya que la base del algoritmo es la probabilidad condicional y no implica un cálculo de distancia. . Al ejecutar el flujo de Rapidminer se obtienen como salidas las siguientes matrices de confusión: . Modelo sin normalización . . Modelo normalizado . . Como conclusión, se verificó para un caso particular, que la estandarización o normalización de los datos no impacta significativamente en la precisión de un modelo Naive Bayes. Esto condice con la teoría mencionada anteriormente sobre la base del modelo. Sin embargo, existen diversos modelos de Machine Learning de clasificación que requieren la normalización para funcionar correctamente, por lo que este experimento no es suficiente para descartar la técnica de normalización en la preparación de los datos. .",
            "url": "https://nachlord996.github.io/Machine-Learning-Portfolio/2021/11/24/Efecto-de-la-estandarizaci%C3%B3n-y-normalizaci%C3%B3n.html",
            "relUrl": "/2021/11/24/Efecto-de-la-estandarizaci%C3%B3n-y-normalizaci%C3%B3n.html",
            "date": " • Nov 24, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Support Vector Machines: Predicción de clase para coordenadas en espacio 2D",
            "content": "La clasificación binaria es un problema recurrente en el ámbito de los modelos supervisados de Machine Learning. Al existir exclusivamente dos clases, la visualización de los datos (Mayoritariamente en dos dimensiones) otorga la capacidad de decidir enfrentar con el problema con un tipo de algoritmos en particular. . En este artículo se demostrará el uso de un algoritmo reciente para clasificación binaria llamado “Support Vector Machines”. El mismo recoge aspectos de la Estadística, la Teoría Matemática de la Optimización y de las Ciencias de la Computación. . Contexto . Para este ejercicio se utilizará un Dataset de ejemplo de coordenadas bidimensionales en el plano. Cada una de ellas tendrá una clase correspondiente, la cual será la variable objetivo a predecir del modelo. El ejercicio fue extraído del libro “Predictive Analytics and Data Mining” (Kotu, 2015). . Coordenada X1 [Numérica] | Coordenada X2 [Numérica] | Clase [Binomial (A - B)] | . El dataset es muy reducido, contiene solamente 17 ejemplos. Por tanto, entrenaremos un modelo de SVM utilizando la totalidad de los datos disponibles. Para testear utilizaremos puntos elegidos estratégicamente que nos permitan valorar su desempeño. . Datos . El dataset a utilizar es el siguiente: . x1 x2 Clase . 1.5 | 2.5 | A | . 2 | 2 | A | . 1 | 2 | A | . 0.75 | 3 | A | . 2 | 1 | A | . 1.75 | 1.75 | A | . 2.75 | 0.75 | A | . 2.5 | 1.5 | A | . 0.5 | 6 | B | . 1.5 | 6 | B | . 2 | 5.5 | B | . 1 | 5.5 | B | . 1 | 6.5 | B | . 2 | 4.5 | B | . 1.75 | 5.25 | B | . 2.75 | 4.25 | B | . 2.5 | 5 | B | . Los datos a utilizar cumplen con los requisitos de SVM. Todos los predictores son numéricos y la variable objetivo es binaria. . Modelo . La implemententación del modelo será en Rapidminer. El primer paso será preparar los datos y cargarlos con un operador Retrieve. Es bastante interesante aprovechar esta oportunidad y visualizar los datos mediantes un gráfico plot. . . Con este gráfico se identifica fácilmente la naturaleza de la clasificación a realizar. Ambas clases se encuentran separadas apropiadamente por una “recta imaginaria”. . Al flujo se debe agregar el operador de Support Vector Machine (SVM) así como también un Apply Model. Este último se nutrirá de ejemplos generados manualmente, los cuales cargaremos en un dataset nuevo. . El flujo debería verse de la siguiente manera: . . Resultados . El nuevo dataset contiene tres ejemplos (X1, X2) para clasificar: (1.5, 1), (1.5, 4) y (2, 7). . Desde un análisis gráfico, los puntos 1 y 3 son fácilmente ubicables en los sectores donde una única clase predomina, sin embargo el punto 2 está en medio de estas zonas, en una frontera donde la predicción se vuelve más compleja. . Por esta razón, la confianza de predicción será más alta para los puntos 1 y 3, mientras que el punto 2 tendrá una confianza más equilibrada entre clases. . . En esta imagen se confirman nuestras suposiciones sobre el comportamiento del modelo. . Adicionalmente, vale la pena revisar los pesos generados por el modelo. Volviendo a la teoría, estos representan los coeficientes de cada predictor para partir de hyperespacio usando hyperplanos. . .",
            "url": "https://nachlord996.github.io/Machine-Learning-Portfolio/svm/rapidminer/2021/11/04/Support-Vector-Machines-Datos-de-prueba-con-Kernel-Lineal.html",
            "relUrl": "/svm/rapidminer/2021/11/04/Support-Vector-Machines-Datos-de-prueba-con-Kernel-Lineal.html",
            "date": " • Nov 4, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Lda Utilizando Python Y Scikit",
            "content": "LDA utilizando Python y Scikit-learn . Los algoritmos lineales son de gran utilizar para atacar problemas relativamente sencillos, donde es notaria una relación lineal entre los predictores y la variable objetivo. . En esta ocasión se utilizará el algoritmo de Análisis de Discriminante Lineal para un problema de clasificación binaria. Para ello, se utilizará la librería de Scikit-learn de Python que provee métodos de entrenamiento para un modelo LDA. . Se cuenta con un Dataset simple de únicamente dos variables: X e Y. Es importante destacar que la cantidad de ejemplos no es muy grande, hay solamente 20 registros. La clase por predecir tiene dos posibles valores 0 y 1. . Importar librerías y lectura de Dataset . Para este proceso se utilizarán las librerías de Scikit-learn, Matplotlib y Pandas. Para evitar complicaciones mayores accederé a un documento de Google Colab y escribiré el código Python en un Jupyter Notebook. . . ## . ## . Graficar Dataset en dos dimensiones . Al disponer de únicamente dos variables, la linealidad del problema puede verse representada mediante un gráfico de tipo “Plot” de ambos predictores. Esto puede ser realizado mediante la librería Matplotlib, manipulando previamente las columnas al invocar el método de graficar. . . En el gráfico se ve claramente la separación de ambas clases en dos grupos disjuntos. Por lo tanto, podemos asegurar que la utilización de un modelo lineal para trabajar este Dataset es apropiada. . Entrenamiento del modelo . Habiendo confirmado la disposición de las clases en el gráfico plot, el siguiente paso implicar entrenar un modelo LDA utilizando el Dataset. Para ello, primero es necesario separar los predictores de la variable objetivo. . . El método train_test_split permite generar cuatro conjuntos a partir del Dataset. Separa los datos en predictores y variable objetivo. Además, aparta algunos datos para utilizar como validación del modelo. En este caso se utilizará el 25% (5 ejemplos) del Dataset para validación. . . Para entrenar el modelo LDA se utilizan los métodos importados de scikit-learn. . . ## . Validación del modelo . Para validar el entrenamiento del modelo, utilizaremos los ejemplos que mantuvimos fuera del entrenamiento. Es importante tener también los valores reales de la variable objetivo para poder compararlos con las predicciones del modelo. . . Los resultados muestran que el modelo realizó una predicción correcta en todos los casos de validación. También es de utilidad examinar la matriz de confusión de las pruebas. . Esto se puede realizar mediante el método confusion_matrix de Scikit-learn Metrics. . . Cada columna representa las predicciones que el modelo realizó, mientras que las filas los valores reales tomados del Dataset de validación. Si todos los valores de la matriz con ceros a excepción de la diagonal principal, el modelo habrá funcionado perfectamente. En caso contrario nos encontraremos con falsos positivos o falsos negativos, situaciones donde el modelo realizó la predicción incorrecta. .",
            "url": "https://nachlord996.github.io/Machine-Learning-Portfolio/2021/11/02/LDA-utilizando-Python-y-Scikit.html",
            "relUrl": "/2021/11/02/LDA-utilizando-Python-y-Scikit.html",
            "date": " • Nov 2, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Preparación De Datos Con Python",
            "content": "Preparación de datos con Python . Actualmente existen múltiples herramientas que permiten una exploración profunda sobre los datos iniciales, las cuales son de gran utilidad para el analista que decide comenzar con este proceso. Desde una planilla de cálculo como Microsoft Excel, hasta módulos especializados en herramientas productivas, existen diversos mecanismos para llevar a cabo la preparación de los datos, teniendo como objetivo generar los insumos para un modelo de Machine Learning. . En esta ocasión, se pretende investigar los distintos mecanismos que dispone Python y las librerías desarrolladas por la comunidad, para el análisis y tratamiento de datos. . Todo el código Python será escrito en un Jupyter Notebook, aprovechando las facilidades que este tipo de documentos ofrece para la ejecución de este. Además, utilizaré la herramienta Google Colab, la cual resuelve las referencias a las librerías de Python a utilizar automáticamente. . Importar librerías y lectura del Dataset . El primer paso consiste en importar todas las librerías a utilizar durante el proyecto. . NumPy . | Pandas . | Seaborn . | MatPlotLib . | . Luego se debe leer el contenido del archivo “Titanic-Dataset.csv”. Al ejecutar el código se puede visualizar una vista previa de los registros. Debería verse de esta manera: . . Visualizar Dataset . Por medio de las siguientes instrucciones es posible identificar las columnas con valores numéricos. El resto de los atributos son de tipo categórico. . . La generación de gráficos para visualizar el Dataset puede realizarse mediante la librería Seaborn. Para ejemplificar, es posible generar un gráfico que ilustre la tasa de sobrevivientes, partiendo de la clase y el sexo del individuo. . . ## . Valores faltantes e imputación . Este Dataset presenta valores faltantes únicamente en los atributos Age, Cabin y Embarked. Para verificar esto, se utiliza el siguiente código: . . Como se puede ver en este conteo de valores faltantes, los atributos más impactados son Age y Cabin. En el caso de edad, será necesario aplicar una imputación de valores, siendo este atributo importante en la predicción del modelo. Probablemente, los atributos Cabin y Embarked no impacten significativamente en el desempeño del modelo, por lo que se podría considerar eliminarlos. . La eliminación de atributos se realiza con el siguiente código. Además se eliminará el número de ticket, el cual no aporta ninguna información para la predicción. . . . Para decidir el valor que será imputado al atributo edad, es conveniente analizar la distribución de este. Esto se puede realizar mediante la librería matplotlib. . . La distribución observada es bastante centrada pero algo inclinada hacia un lado, por lo que una buena decisión sería imputar la edad utilizando la mediana de los datos. Esto se realiza mediante el siguiente código: . . Al concluir este paso, tenemos asegurado que los datos no contienen datos faltantes. Por lo que serán de mayor utilidad y precisión a la hora de entrenar el modelo. .",
            "url": "https://nachlord996.github.io/Machine-Learning-Portfolio/2021/10/29/Preparaci%C3%B3n-de-datos-con-Python.html",
            "relUrl": "/2021/10/29/Preparaci%C3%B3n-de-datos-con-Python.html",
            "date": " • Oct 29, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Árboles De Decisión Para Problemas De Regresión, Housing Uci",
            "content": "Al igual que los problemas de clasificación vistos en artículos anteriores, los árboles de decisión son una herramienta muy intuitiva para atacar los problemas supervisados de regresión. En este tipo de problemas, el objetivo será predecir el valor de una variable continua a partir de un modelo generado con predictores mayormente numéricos. . Los árboles de decisión están dentro del grupo de Algoritmos No Lineales, los cuales hacen menos suposiciones acerca de la estructura de los datos y/o la naturaleza del problema. También cuentan con la ventaja de una mayor interpretabilidad del modelo generado. Un árbol de decisión se representa frecuentemente como un árbol binario donde cada nodo representa una condición a evaluar con respecto a un predictor en particular, y cada hoja alberga un posible valor de la variable de salida. Esta representación permite una rápida evaluación de un nuevo ejemplo para poder obtener la predicción con respecto al mismo. . Por tanto, la complejidad de los árboles de decisión radica en la configuración de los parámetros del modelo, los cuales permiten generar distintas versiones del árbol en base a los valores escogidos. . Criterio . | Profundidad máxima . | Confianza . | Ganancia mínima . | Mínimo tamaño de hoja . | Otros . | . Contexto . Para este ejercicio se utilizará un Dataset provisto por la Univeridad de California Irvine (UCI) de precios de inmuebles bajo el nombre de distribución “Housing”. Este es un Dataset clásico utilizado en problemas de regresión, donde se intenta predecir la mediana de casas ocupadas por los dueños. . Datos . El mismo cuenta con 12 predictores numéricos continuos y 1 binomial. Los atributos del dataset son los siguientes: . CRIM – Proporción de crimen per cápita . | ZN – Proporción de terreno residencial ubicado en solares mayors a 25.000 metros cuadrados. . | INDUS – Proporción de acres de negocios no-retail por pueblo . | CHAS – Contacto con el río Charles { Sí, No} . | NOX – Concentración de óxido nítrico . | RM – Número promedio de habitacione . | AGE – Proporcion de unidades ocupadas por dueños que fueron construidas antes de 1940 . | DIS – Distancias ponderadas a 5 Centros de empleo de Boston . | RAD – Índice de accesibilidad a autopistas periféricas . | TAX – Proporción de impuestos del valor total por cada 10000 USD . | PTRATIO – Proporción de Profesor y Estudiantes en pueblo . | B - Proporción de gente negra . | LSTAT – Porcentaje de estatus más bajo de la población . | . El Dataset comprende 506 valores únicos y se encuentra totalmente completo, es decir ningún predictor contiene valores faltantes. . Modelo . Tras analizar la completitud y buen estado de los datos, se debe implementar el modelo y analizar sus resultados. Para ello, haré un flujo nuevo en Rapidminer que utilice el operador de Árbol de decisión. A partir del Dataset “Housing”, generaré dos conjuntos según la regla 70/30. El primero será destinado a entrenamiento y el segundo a testing del modelo. . Además, utilizaré en esta ocasión un K-Fold Cross Validation para obtener un resultado más realista con respecto a este Dataset que es de un tamaño reducido. Este operador nos permitirá dividir el Dataset en K particiones o “Folds”, entrenar K modelos distintos, reservando cada una de esas particiones para test en cada iteración. . El flujo en Rapidminer se debería ver de la siguiente manera: . . . Al ejecutar este modelo, Rapidminer obtenemos como resultados como la visualización del árbol generado que podrá darnos más información sobre el ajuste de los parámetros. También el vector de Performance indicando los valores de Raíz cuadrada del error medio (RMSE), el error absoluto y el error medio de predicción. . Se analizaron distintos valores de hyperparámetros para el árbol de decisión de regresión para obtener distintos resultados. . A continuación, se registran los mejores valores encontrados por método de tanteo y observación. . Profundidad Mejora mínima Tamaño de hoja mínimo Alternativas de prepruning RMSE Error absoluto Error relativo . 10 | 0.01 | 5 | 10 | 4.795 | 3.009 +/- 3.734 | 14.11% +/- 14.90% | .",
            "url": "https://nachlord996.github.io/Machine-Learning-Portfolio/2021/10/18/%C3%81rboles-de-Decisi%C3%B3n-para-problemas-de-regresi%C3%B3n,-Housing-UCI.html",
            "relUrl": "/2021/10/18/%C3%81rboles-de-Decisi%C3%B3n-para-problemas-de-regresi%C3%B3n,-Housing-UCI.html",
            "date": " • Oct 18, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Direccionamiento De Una Campaña De Marketing Para Ereaders",
            "content": "En múltiples ocasiones, los equipos de marketing deberán estimar la mejor estrategia para vender un nuevo producto. Especialmente cuando se trata de una marca establecida, direccionar la estrategia para los clientes potenciales determina el éxito del lanzamiento del producto. Estos casos son especiales, ya que es muy alta la probabilidad de que se disponga de datos sobre los productos anteriormente lanzados. . En base a datos históricos, es posible entrenar un modelo que permita determinar si un cliente comprará el nuevo producto cuando este sea anunciado. En este caso, se estudiará un Dataset con información de los patrones de comportamiento de los clientes en sus actividades en el sitio web de una empresa de E-Readers. . Contexto . Se dispone de dos Dataset con información de los clientes. Uno de ellos está completamente etiquedado de acuerdo con las compras realizadas de los clientes. La variable objetivo a predecir es del tipo nominal, por lo que tiene múltiples y determinados valores posibles. . Por su parte, el segundo Dataset no contiene esta variable, el mismo corresponde a información actual de los perfiles de los clientes. A partir de éste, se deberán predecir las clases en la etapa de testeo del modelo. . Naturalmente este tipo de problema se tratará con un modelo supervisado de clasificación, ya que es de nuestro interés predecir la clase de clientes sin clasificar. . Para este ejercicio, se utilizará un modelo de Árbol de Decisión en Rapidminer. El objetivo será examinar y comparar el funcionamiento de este tipo de algoritmo no lineal para un ejercicio de clasificación. . Datos . Los árboles de decisión son modelos de Machine Learning que requieren poca preparación de los datos, ya que son bastantes permisivos con valores faltantes, distribuciones sesgadas y distintos tipos de datos. . Con respecto a este último punto, la siguiente tabla muestra información sobre los predictores disponibles: . Predictor Rango Comentario . ID | - | Identificador del cliente | . Edad | - | Variable numérica | . EstadoCivil | C - S | Variable binomial | . Sexo | F – M | Variable binomial | . ActividadWebsite | Escasa – Regular - Frecuente | Variable nominal | . ComproElectronicos12 | SI – NO | Variable binomial | . MiroElectronicos12 | SI – NO | Variable binomial | . ComproMedios18 | SI – NO | Variable binomial | . ComproLibrosDigitales | SI - NO | Variable binomial | . MetodoPago | Transferencia Bancaria – Cuenta Website – Tarjeta Credito – Debito Mensual | Variable nominal | . En este caso, la mayoría de variables son categóricas. Considero que todos los datos proporcionados son relevantes para el ejercicio, a excepción del identificador del cliente. Este dato no es de utilidad para la predicción, ya que se trata de una referencia a un registro en particular. . Este tipo de datos puede ser excluido del Dataset desde el inicio. Sin embargo, en este caso usaré una ventaja que aporta Rapidminer, cambiaré el tipo de dato a “Identificador”. De esta manera, se mantendrá a lo largo del flujo, pero no influirá en el entrenamiento del modelo. . . Por medio del operador “Set Role” es posible modificar el papel de ese atributo en Rapidminer. En la siguiente imagen se puede ver como son diferenciados de los demás. . . Modelo . Agregamos un operador de Árbol de Decisión en el flujo de Rapidminer. El mismo usará el dataset de entrenamiento como entrada. El modelo resultante será probado mediante el operador “Apply Model”, usando el Dataset de Test. . El flujo resultante se ve de la siguiente manera: . . Al ejecutar el modelo con los datos de prueba, vemos que Rapidminer agrega 4 atributos de confianza y un atributo adicional para la predicción de la clase objetivo. . Cada una de estas confianzas indican que tan probable es que el modelo clasifique el registro en cada una de las 4 clases. Por tanto, la suma de todas las confianzas da como resultado 1. . . La probabilidad obtenida para cada registro corresponde a la proporción de ejemplos de clases distintas que durante el entrenamiento se generaron en el mismo nodo hoja. . Queda claro que aquellos registros con confianzas más equilibrados tienen menor seguridad de predicción frente a aquellos donde las diferencias son marcadas. . A continuación, voy a seleccionar 3 registros en particular y aplicar modificaciones en los hyperparámetros del árbol de decisión. Esta prueba debería arrojar resultados distintos de clase objetivo, especialmente si los registros elegidos tienen confianzas equilibradas. . Se ejecutarán 4 configuraciones distintas con los siguientes valores de parámetros . Criterio: Gain_Ratio Maximal_Depth: 10 Minimal_Gain: 0.01 . | Criterio: Gini_Index Maximal_Depth: 10 Minimal_Gain: 0.01 . | Criterio: Gain_Ratio Maximal_Depth: 5 Minimal_Gain: 0.05 . | Criterio: Gain_Ratio Maximal_Depth: 20 Minimal_Gain: 0.05 . | ID Cliente Predicción Test 1 Predicción Test 2 Predicción Test 3 Predicción Test 4 . 98200 | Adoptante Temprano 54% | Adoptante Temprano 100% | Adoptante Temprano 41% | MayoriaTardia 84% | . 76655 | MayoriaTardia 84% | Adoptante Temprano 50% | MayoriaTardia 75% | Adoptante Temprano 100% | . 63570 | Adoptante Temprano 100% | Adoptante Temprano 75% | Adoptante Temprano 100% | Adoptante Temprano 100% | . Como conlusión, los parámetros que establecen las distintas operaciones a efectuar sobre el árbol de decisión generan resultados muy distintos entre sí, cambiando la confianza de predicción de algunos registros, incluso la clase predicha en sí. . Este tipo de algoritmo de Machine Learning es fácil de entender, ya que su salida es una representación que podemos entender. El modelo genera un árbol donde en cada nodo se deberá tomar una decisión con respecto a un predictor dado y en sus hojas estarán las clases objetivo a predecir. Por tanto, predecir un nuevo valor implica simplemente recorrer el árbol generado. .",
            "url": "https://nachlord996.github.io/Machine-Learning-Portfolio/2021/10/16/Direccionamiento-de-una-campa%C3%B1a-de-marketing-para-eReaders.html",
            "relUrl": "/2021/10/16/Direccionamiento-de-una-campa%C3%B1a-de-marketing-para-eReaders.html",
            "date": " • Oct 16, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Sobre mi",
          "content": "Soy Ignacio Martínez, un estudiante avanzado de Ingeniería en Informática con un interés particular por las nuevas tecnologías. En este portafolio pretendo publicar todos mis trabajos significativos realizados en el área del Aprendizaje Automático. . Trabajo a tiempo parcial como Analista en Sistemas, tengo experiencia en Desarrollo Web, Business Intelligence e intregraciones por API. . Puedes encontrar el resto de mis trabajos en Github. .",
          "url": "https://nachlord996.github.io/Machine-Learning-Portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nachlord996.github.io/Machine-Learning-Portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}