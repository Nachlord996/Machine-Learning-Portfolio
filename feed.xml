<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://nachlord996.github.io/Machine-Learning-Portfolio/feed.xml" rel="self" type="application/atom+xml" /><link href="https://nachlord996.github.io/Machine-Learning-Portfolio/" rel="alternate" type="text/html" /><updated>2021-12-01T16:43:09-06:00</updated><id>https://nachlord996.github.io/Machine-Learning-Portfolio/feed.xml</id><title type="html">Ignacio Martínez</title><subtitle>En este portafolio están todos mis proyectos relacionados con Métodos de Aprendizaje Automático</subtitle><entry><title type="html">Un problema clásico de clasificación: UCI Iris mediante K-NN</title><link href="https://nachlord996.github.io/Machine-Learning-Portfolio/k%20nearest%20neighbours/rapidminer/no%20lineal/2021/12/01/KNN-Clasificaci%C3%B3n-del-Dataset-Iris.html" rel="alternate" type="text/html" title="Un problema clásico de clasificación: UCI Iris mediante K-NN" /><published>2021-12-01T00:00:00-06:00</published><updated>2021-12-01T00:00:00-06:00</updated><id>https://nachlord996.github.io/Machine-Learning-Portfolio/k%20nearest%20neighbours/rapidminer/no%20lineal/2021/12/01/KNN%20Clasificaci%C3%B3n%20del%20Dataset%20Iris</id><author><name></name></author><category term="K Nearest Neighbours" /><category term="Rapidminer" /><category term="No Lineal" /><summary type="html">Dentro de las técnicas para clasificación en problemas no lineales, existe un grupo que utiliza la distancia entre predictores como elemento central. Esto parte de la idea de que los ejemplos que pertenecen a una misma clase, serán ciertamente más similares en cuanto a valores de los distintos predictores.</summary></entry><entry><title type="html">Validación de un modelo: Respuesta a una campaña de mail usando Naïve Bayes</title><link href="https://nachlord996.github.io/Machine-Learning-Portfolio/na%C3%AFve%20bayes/validaci%C3%B3n%20del%20modelo/roc/lift%20charts/2021/11/28/Respuesta-a-una-campa%C3%B1a-de-mail-usando-Naive-Bayes.html" rel="alternate" type="text/html" title="Validación de un modelo: Respuesta a una campaña de mail usando Naïve Bayes" /><published>2021-11-28T00:00:00-06:00</published><updated>2021-11-28T00:00:00-06:00</updated><id>https://nachlord996.github.io/Machine-Learning-Portfolio/na%C3%AFve%20bayes/validaci%C3%B3n%20del%20modelo/roc/lift%20charts/2021/11/28/Respuesta%20a%20una%20campa%C3%B1a%20de%20mail%20usando-Naive-Bayes</id><author><name></name></author><category term="Naïve Bayes" /><category term="Validación del modelo" /><category term="ROC" /><category term="Lift Charts" /><summary type="html">La calidad de un modelo de Machine Learning no se evalúa exclusivamente con los resultados que este genera durante su entrenamiento y validación. La aplicabilidad de cualquier modelo debe considerar múltiples métricas que demuestren su efectividad en la realidad.</summary></entry><entry><title type="html">Caso de Estudio: Detección de enfermedad cardíaca</title><link href="https://nachlord996.github.io/Machine-Learning-Portfolio/2021/11/25/Caso-de-Estudio-Predicci%C3%B3n-de-enfermedad-del-coraz%C3%B3n.html" rel="alternate" type="text/html" title="Caso de Estudio: Detección de enfermedad cardíaca" /><published>2021-11-25T00:00:00-06:00</published><updated>2021-11-25T00:00:00-06:00</updated><id>https://nachlord996.github.io/Machine-Learning-Portfolio/2021/11/25/Caso%20de%20Estudio%20-%20Predicci%C3%B3n%20de%20enfermedad%20del%20coraz%C3%B3n</id><author><name></name></author><summary type="html">Contexto</summary></entry><entry><title type="html">Efecto De La Estandarización Y Normalización</title><link href="https://nachlord996.github.io/Machine-Learning-Portfolio/2021/11/24/Efecto-de-la-estandarizaci%C3%B3n-y-normalizaci%C3%B3n.html" rel="alternate" type="text/html" title="Efecto De La Estandarización Y Normalización" /><published>2021-11-24T00:00:00-06:00</published><updated>2021-11-24T00:00:00-06:00</updated><id>https://nachlord996.github.io/Machine-Learning-Portfolio/2021/11/24/Efecto-de-la-estandarizaci%C3%B3n-y-normalizaci%C3%B3n</id><author><name></name></author><summary type="html">Efecto de la estandarización y normalización</summary></entry><entry><title type="html">Support Vector Machines: Predicción de clase para coordenadas en espacio 2D</title><link href="https://nachlord996.github.io/Machine-Learning-Portfolio/svm/rapidminer/2021/11/04/Support-Vector-Machines-Datos-de-prueba-con-Kernel-Lineal.html" rel="alternate" type="text/html" title="Support Vector Machines: Predicción de clase para coordenadas en espacio 2D" /><published>2021-11-04T00:00:00-05:00</published><updated>2021-11-04T00:00:00-05:00</updated><id>https://nachlord996.github.io/Machine-Learning-Portfolio/svm/rapidminer/2021/11/04/Support-Vector-Machines-Datos-de-prueba-con-Kernel-Lineal</id><author><name></name></author><category term="SVM" /><category term="Rapidminer" /><summary type="html">La clasificación binaria es un problema recurrente en el ámbito de los modelos supervisados de Machine Learning. Al existir exclusivamente dos clases, la visualización de los datos (Mayoritariamente en dos dimensiones) otorga la capacidad de decidir enfrentar con el problema con un tipo de algoritmos en particular.</summary></entry></feed>